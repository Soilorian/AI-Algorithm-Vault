{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eb3cde",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "Spring 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 2 - Minimax Algorithm<br>\n",
    "<font color=696880 size=4>\n",
    "Sepehr Harfi\n",
    "\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a011cf",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d506013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:00:22.178362Z",
     "start_time": "2024-04-08T20:00:21.899808Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set your student number and name\n",
    "Student_number = \"401106266\"\n",
    "Name = \"Mahdi\"\n",
    "Last_name = \"Ali nejad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de0ad2",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf03f32",
   "metadata": {},
   "source": [
    "<font color=red>\n",
    "Please run all the cells.\n",
    "\n",
    "Please refrain from making any changes to the existing files, such as `board.py`, `game.py`, etc. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f884c",
   "metadata": {},
   "source": [
    "#### **Note:** It is strongly recommended to execute this notebook on your local device instead of Google Colab due to limitations with graphics. However, if you still prefer using Google Colab, you can refer to this [link](https://vishnudsharma.medium.com/visualizing-tkinter-and-pygame-in-colab-272c5a245f8c) for assistance in using graphics with Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fdd74",
   "metadata": {},
   "source": [
    "# Libraries & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94534a71",
   "metadata": {},
   "source": [
    "**Dont change the below cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4bb0af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:00:22.193609Z",
     "start_time": "2024-04-08T20:00:22.183374Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_working_directory = os.getcwd()\n",
    "src_directory_path = os.path.join(curr_working_directory, \"src\")\n",
    "\n",
    "if os.path.exists(src_directory_path):\n",
    "    os.chdir(src_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bcdc39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:00:22.901160Z",
     "start_time": "2024-04-08T20:00:22.871127Z"
    }
   },
   "outputs": [],
   "source": [
    "import game\n",
    "from player import Player\n",
    "from random_player import RandomPlayer\n",
    "from random_greedy_player import RandomGreedyPlayer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf190d9",
   "metadata": {
    "id": "5cf190d9"
   },
   "source": [
    "\n",
    "# Breakthrough Game: Minimax and AlphaBeta Players (100 Points)\n",
    "\n",
    "In this notebook, we will implement and compare two AI strategies, Minimax and AlphaBeta, for playing a simple version of the Breakthrough game. We aim to assess the performance of these strategies in terms of their execution time and win probability in matches against a Greedy player and against each other with varying depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba059eaf",
   "metadata": {},
   "source": [
    "# Breakthrough Game Rules\n",
    "\n",
    "The Breakthrough game is a two-player strategy board game played on an 6x6 grid. The objective of the game is to move your pieces from one end of the board to the other before your opponent does.\n",
    "\n",
    "## Game Setup\n",
    "\n",
    "- The board consists of a 6x6 grid with alternating dark and light squares.\n",
    "- Each player starts with 12 pieces placed on their respective sides of the board.\n",
    "- The pieces are initially arranged in two rows, with each row containing 6 pieces.\n",
    "\n",
    "## Piece Movement\n",
    "\n",
    "- Each player can only move their own pieces.\n",
    "- Pieces can move diagonally or straight forward one square at a time.\n",
    "- Pieces cannot move backward or sideways.\n",
    "- Pieces can capture their opponent's pieces by moving diagonally forward and landing on a square occupied by an opponent's piece.\n",
    "- Captured pieces are removed from the board.\n",
    "\n",
    "\n",
    "## Game End\n",
    "\n",
    "- If a player successfully moves one of their pieces to the opposite end of the board, they win the game.\n",
    "\n",
    "## Additional Rules\n",
    "\n",
    "- Players can only capture the opponent's piece if its in their diagonal forward squares and they cannot have two or more pieces of their color in the same square.\n",
    "- Players take turns moving their pieces.\n",
    "- Players must make a move on their turn, and they cannot skip their turn.\n",
    "- If a player has multiple legal moves, they can choose which piece to move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe681eb",
   "metadata": {
    "id": "ffe681eb"
   },
   "source": [
    "\n",
    "## Demo of the game with graphics\n",
    "\n",
    "Initially, using the cell below, you can see a demo gameplay of two Random Greedy Players to gain insights into the game. Additionally, you can explore the gameplay of other agents such as \"Player\" which plays in a complete greedy way and also \"RandomPlayer\", to further enhance your understanding of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c41b4",
   "metadata": {},
   "source": [
    "**Note:** You can use the cell below anywhere you want to see the game with the graphics. For this purpose, you should only be careful to pass the right class of players to the `game.Game()`. Also, If you want to see the performance of your Minimax or AlphaBeta agents, you may need to add their classes as a python file to the `src` folder and import their classes in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b067d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b067d36",
    "outputId": "c31c275e-abbe-4383-c984-80922eb2d829",
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:17.068709Z",
     "start_time": "2024-04-01T18:22:56.561781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -178.0, 1: 220.0}\r\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "# Change the above line to %%python if youre using Windows OS\n",
    "from src import game\n",
    "from src.player import Player\n",
    "from src.random_greedy_player import RandomGreedyPlayer\n",
    "from src.random_player import RandomPlayer\n",
    "\n",
    "random_greedy_game = game.Game(RandomPlayer, RandomGreedyPlayer)\n",
    "print(random_greedy_game.play(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac79cb",
   "metadata": {
    "id": "d6ac79cb"
   },
   "source": [
    "\n",
    "## Minimax Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee44bc1",
   "metadata": {},
   "source": [
    "To gain insights into the game, start by reading the `board.py`, `game.py`, and `player.py` files to understand the game implementation. Then, implement a minimax agent. Recall that the minimax algorithm evaluates game states and selects optimal moves. Compare the performance of the random greedy and minimax agents to gain insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c64f4",
   "metadata": {},
   "source": [
    "**Note:** To implement the Minimax agent, you should only modify the `MinimaxPlayer` class in the cell below. You can either use the Board class `get_scores` function or define your own score funcion here and use it for the evaluation of a board state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd51e9",
   "metadata": {},
   "source": [
    "**Note:** Your minimax implementation should have a depth instance which determines the level to which the algorithm should be executed for each move. It controls the extent of the search tree explored by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063de25",
   "metadata": {},
   "source": [
    "**Note:** Feel free to add cells if you need to, but don't erase the existing cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf52f5b",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1a446c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:02:01.102630Z",
     "start_time": "2024-04-08T20:02:01.078963Z"
    }
   },
   "outputs": [],
   "source": [
    "class MinimaxPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_next_move(self):\n",
    "        # TODO: Implement this function based on the minimax algorithm\n",
    "        self.board.start_imagination()\n",
    "        board = self.board.get_imaginary_board()\n",
    "        move, score = self.maximize(board)\n",
    "        if not self.board.is_move_valid(self.board.get_board_grid(), move, self.player_number):\n",
    "            raise Exception(\"Invalid move\")\n",
    "        return move\n",
    "\n",
    "    def get_range(self, pn):\n",
    "        range_n = 0, self.board.get_n()\n",
    "        step = 1\n",
    "        if pn == 0:\n",
    "            range_n = self.board.get_n() - 1, -1\n",
    "            step = -1\n",
    "        return range_n[0], range_n[1], step\n",
    "\n",
    "    def maximize(self, state, cur_depth=1):\n",
    "        if cur_depth == self.depth:\n",
    "            return self.best_move(state)\n",
    "\n",
    "        actions = []\n",
    "        r = self.get_range(self.player_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.player_number:\n",
    "                    for direction in self.board.get_possible_directions(self.player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(state)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                            if game_over == self.player_number:\n",
    "                                scores = {self.opponent_number: float('-inf'), self.player_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            elif game_over == self.opponent_number:\n",
    "                                scores = {self.player_number: float('-inf'), self.opponent_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            else:\n",
    "                                actions.append((move, self.minimize(self.board.imaginary_board_grid, cur_depth + 1)[1]))\n",
    "\n",
    "        action = max(actions, key=lambda x: x[1][self.player_number])\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def minimize(self, state, cur_depth=1):\n",
    "        if cur_depth == self.depth:\n",
    "            return self.best_move_op(state)\n",
    "\n",
    "        actions = []\n",
    "        r = self.get_range(self.opponent_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.opponent_number:\n",
    "                    for direction in self.board.get_possible_directions(self.opponent_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(state)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.opponent_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.opponent_number)\n",
    "                            if game_over == self.player_number:\n",
    "                                scores = {self.opponent_number: float('-inf'), self.player_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            elif game_over == self.opponent_number:\n",
    "                                scores = {self.player_number: float('-inf'), self.opponent_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            else:\n",
    "                                actions.append((move, self.maximize(self.board.get_imaginary_board(), cur_depth + 1)[1]))\n",
    "\n",
    "        action = max(actions, key=lambda x: x[1][self.opponent_number])\n",
    "        return action\n",
    "\n",
    "    def best_move(self, board):\n",
    "        max_score = -float('inf')\n",
    "        best_scores = None\n",
    "        best_move = None\n",
    "        r = self.get_range(self.player_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.player_number:\n",
    "                    for direction in self.board.get_possible_directions(self.player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(board)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                            if scores[self.player_number] > max_score:\n",
    "                                max_score = scores[self.player_number]\n",
    "                                best_scores = scores\n",
    "                                best_move = move\n",
    "        return best_move, best_scores\n",
    "\n",
    "\n",
    "    def best_move_op(self, board):\n",
    "        max_score = -float('inf')\n",
    "        best_scores = None\n",
    "        best_move = None\n",
    "        r = self.get_range(self.opponent_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.opponent_number:\n",
    "                    for direction in self.board.get_possible_directions(self.opponent_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(board)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.opponent_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.opponent_number)\n",
    "                            if scores[self.opponent_number] > max_score:\n",
    "                                max_score = scores[self.opponent_number]\n",
    "                                best_scores = scores\n",
    "                                best_move = move\n",
    "        return best_move, best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb6b4c",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c148af",
   "metadata": {},
   "source": [
    "Now play the game 5 times between two random players and calculate the average execution time. Also do this for the game of a random player and a minimax player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27ce0cf",
   "metadata": {
    "id": "c27ce0cf",
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:24.150523Z",
     "start_time": "2024-04-01T18:23:17.091448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Players Average Execution Time: 0.003999137878417968 seconds\n",
      "Minimax Player and Random Player Average Execution Time: 1.4065239906311036 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print the average execution time of two random players\n",
    "mean_time_random = 0\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    test = game.Game(RandomPlayer, RandomPlayer)\n",
    "    test.play()\n",
    "    total_time = time.time() - time_start\n",
    "    mean_time_random += total_time\n",
    "\n",
    "mean_time_random /= n\n",
    "\n",
    "print(\"Random Players Average Execution Time:\", mean_time_random, \"seconds\")\n",
    "\n",
    "# TODO: calculate and print the average execution time of a random player and a minimax players with depth=3 \n",
    "mean_time_minimax = 0\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    test = game.Game(MinimaxPlayer, RandomPlayer)\n",
    "    test.play(False)\n",
    "    total_time = time.time() - time_start\n",
    "    mean_time_minimax += total_time\n",
    "\n",
    "mean_time_minimax /= n\n",
    "print(\"Minimax Player and Random Player Average Execution Time:\", mean_time_minimax, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46ed00",
   "metadata": {},
   "source": [
    "## Alphabeta Agent (30 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b1458",
   "metadata": {},
   "source": [
    "As we can see from the above code, the Minimax algorithm takes much longer time to execute. To improve the execution time, we can implement the AlphaBeta pruning algorithm. In the next cell, we will be implementing the AlphaBeta player.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa687f",
   "metadata": {},
   "source": [
    "### Implementation (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e20ed6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:24.167643Z",
     "start_time": "2024-04-01T18:23:24.151529Z"
    }
   },
   "outputs": [],
   "source": [
    "class AlphaBetaPlayer(Player):\n",
    "    def __init__(self, player_number, board, depth=3):\n",
    "        super().__init__(player_number, board)\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_next_move(self):\n",
    "        self.board.start_imagination()\n",
    "        board = self.board.get_imaginary_board()\n",
    "        move, score = self.maximize(board, float('-inf'), float('inf'))\n",
    "        if not self.board.is_move_valid(self.board.get_board_grid(), move, self.player_number):\n",
    "            raise Exception(\"Invalid move\")\n",
    "        return move\n",
    "\n",
    "    def get_range(self, pn):\n",
    "        if pn == 0:\n",
    "            return self.board.get_n() - 1, -1, -1\n",
    "        else:\n",
    "            return 0, self.board.get_n(), 1\n",
    "\n",
    "    def maximize(self, state, alpha, beta, cur_depth=1):\n",
    "        if cur_depth == self.depth:\n",
    "            return self.best_move(state)\n",
    "\n",
    "        actions = []\n",
    "        r = self.get_range(self.player_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.player_number:\n",
    "                    for direction in self.board.get_possible_directions(self.player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(state)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                            if game_over == self.player_number:\n",
    "                                scores = {self.opponent_number: float('-inf'), self.player_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            elif game_over == self.opponent_number:\n",
    "                                scores = {self.player_number: float('-inf'), self.opponent_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            else:\n",
    "                                minimize = self.minimize(self.board.get_imaginary_board(), alpha, beta, cur_depth + 1)\n",
    "                                actions.append((move, minimize[1]))\n",
    "                            minimize = actions[-1]\n",
    "                            if minimize[1][self.player_number] >= beta:\n",
    "                                return minimize\n",
    "                            alpha = max(alpha, minimize[1][self.player_number])\n",
    "\n",
    "        action = max(actions, key=lambda x: x[1][self.player_number])\n",
    "        return action\n",
    "    \n",
    "    def minimize(self, state, alpha, beta, cur_depth=1):\n",
    "        if cur_depth == self.depth:\n",
    "            return self.best_move_op(state)\n",
    "\n",
    "        actions = []\n",
    "        r = self.get_range(self.opponent_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.opponent_number:\n",
    "                    for direction in self.board.get_possible_directions(self.opponent_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(state)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.opponent_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.opponent_number)\n",
    "                            if game_over == self.player_number:\n",
    "                                scores = {self.opponent_number: float('-inf'), self.player_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            elif game_over == self.opponent_number:\n",
    "                                scores = {self.player_number: float('-inf'), self.opponent_number: float('inf')}\n",
    "                                actions.append((move, scores))\n",
    "                            else:\n",
    "                                maximize = self.maximize(self.board.get_imaginary_board(), alpha, beta, cur_depth + 1)\n",
    "                                actions.append((move, maximize[1]))\n",
    "                            maximize = actions[-1]\n",
    "                            if maximize[1][self.opponent_number] <= alpha:\n",
    "                                return maximize\n",
    "                            beta = min(beta, maximize[1][self.opponent_number])\n",
    "\n",
    "        action = max(actions, key=lambda x: x[1][self.opponent_number])\n",
    "        return action\n",
    "\n",
    "    def best_move(self, board):\n",
    "        max_score = -float('inf')\n",
    "        best_scores = None\n",
    "        best_move = None\n",
    "        r = self.get_range(self.player_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.player_number:\n",
    "                    for direction in self.board.get_possible_directions(self.player_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(board)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.player_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.player_number)\n",
    "                            if scores[self.player_number] > max_score:\n",
    "                                max_score = scores[self.player_number]\n",
    "                                best_scores = scores\n",
    "                                best_move = move\n",
    "        return best_move, best_scores\n",
    "\n",
    "    def best_move_op(self, board):\n",
    "        max_score = -float('inf')\n",
    "        best_scores = None\n",
    "        best_move = None\n",
    "        r = self.get_range(self.opponent_number)\n",
    "        for i in range(r[0], r[1], r[2]):\n",
    "            for j in range(r[0], r[1], r[2]):\n",
    "                if self.board.get_board_grid()[i][j] == self.opponent_number:\n",
    "                    for direction in self.board.get_possible_directions(self.opponent_number):\n",
    "                        move = (i, j), (i + direction[0], j + direction[1])\n",
    "                        self.board.imaginary_board_grid = np.copy(board)\n",
    "                        if self.board.is_move_valid(self.board.get_imaginary_board(), move, self.opponent_number):\n",
    "                            scores, game_over = self.board.place_piece_imaginary(move, self.opponent_number)\n",
    "                            if scores[self.opponent_number] > max_score:\n",
    "                                max_score = scores[self.opponent_number]\n",
    "                                best_scores = scores\n",
    "                                best_move = move\n",
    "        return best_move, best_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703a67f",
   "metadata": {},
   "source": [
    "### Time Analysis (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83174e",
   "metadata": {},
   "source": [
    "Now play the game 5 times between the random player and an alphabeta player and calculate the average execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce1a47c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:26.383486Z",
     "start_time": "2024-04-01T18:23:24.168650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabeta Player and Random Player Average Execution Time: 0.441999626159668 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO: calculate and print the average execution time of a random player and an alpha-beta players with depth=3 \n",
    "mean_time_alphabeta = 0\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    time_start = time.time()\n",
    "    test = game.Game(AlphaBetaPlayer, RandomPlayer)\n",
    "    test.play(False)\n",
    "    total_time = time.time() - time_start\n",
    "    mean_time_alphabeta += total_time\n",
    "\n",
    "mean_time_alphabeta /= n\n",
    "\n",
    "print(\"Alphabeta Player and Random Player Average Execution Time:\", mean_time_alphabeta, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480857e5",
   "metadata": {},
   "source": [
    "##### **Note:** The alphabeta agent should be approximately 5X-10X faster than the minimax player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6770103",
   "metadata": {
    "id": "c6770103"
   },
   "source": [
    "\n",
    "## Win Probability Analysis (35 Points)\n",
    "\n",
    "In this section, We will simulate 50 games for your AlphaBeta player against other players and analyze their win rates. Additionally, we will have AlphaBeta players with different depths play against each other. \n",
    "\n",
    "Assume you are always the second player and the black player will always start first. \n",
    "\n",
    "If the agent is implemented correctly, with a depth of two or more it should win all the mentioned agents with a win rate > 0.7.\n",
    "\n",
    "**Note:** You can use the `bulk_play` function from `game.py` for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb476097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:26.659170Z",
     "start_time": "2024-04-01T18:23:26.384494Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "depths = [1, 2, 3]  # List of different second_player_depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e54af5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:26.664330Z",
     "start_time": "2024-04-01T18:23:26.660193Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_results_different_depths(first_player_class, depths, first_player_depth=None, n=50):\n",
    "    results = []\n",
    "    sample_game = game.Game(first_player_class, AlphaBetaPlayer)\n",
    "\n",
    "    for depth in depths:\n",
    "        if first_player_depth is not None:\n",
    "            result = sample_game.bulk_play(n, first_player_depth=first_player_depth, second_player_depth=depth)\n",
    "        else:\n",
    "            result = sample_game.bulk_play(n, second_player_depth=depth)\n",
    "        win_rate = result['white'] / n  # Calculate win rate\n",
    "        results.append({'Depth': depth, 'Win Rate': win_rate})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5361f9",
   "metadata": {},
   "source": [
    "### Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8139c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:23:51.766458Z",
     "start_time": "2024-04-01T18:23:26.665339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       1.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea0e7f",
   "metadata": {},
   "source": [
    "### Random Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e22180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:24:19.929187Z",
     "start_time": "2024-04-01T18:23:51.767463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1      0.98\n",
      "1      2      1.00\n",
      "2      3      1.00\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(RandomGreedyPlayer, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e46530",
   "metadata": {},
   "source": [
    "### Greedy Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37408be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T18:25:09.482933Z",
     "start_time": "2024-04-01T18:24:19.931195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      1       0.0\n",
      "1      2       1.0\n",
      "2      3       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(Player, depths)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3218a4f",
   "metadata": {},
   "source": [
    "### Alphabeta Player with depth 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c7374",
   "metadata": {},
   "source": [
    "**Note:** In this part each game may take up to ~45 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd374cc",
   "metadata": {
    "id": "3cd374cc",
    "ExecuteTime": {
     "end_time": "2024-04-01T18:42:59.026835Z",
     "start_time": "2024-04-01T18:25:09.483941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Depth  Win Rate\n",
      "0      2       1.0\n",
      "1      3       1.0\n",
      "2      4       1.0\n"
     ]
    }
   ],
   "source": [
    "results = compute_results_different_depths(AlphaBetaPlayer, [2, 3, 4], first_player_depth=2, n=30)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873b294",
   "metadata": {
    "id": "4873b294"
   },
   "source": [
    "\n",
    "## Conclusion (5 Points)\n",
    "\n",
    "#### Based on the results of the AlphaBeta player with other players, what is your conclusion?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7783dc",
   "metadata": {},
   "source": [
    "**Answer:** AlphaBeta not only achieves a closer play time to greedy and random player, it also is superior to them and with using more time to calculate it can preform better"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
